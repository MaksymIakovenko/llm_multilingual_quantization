{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e266eca-b017-461f-9be4-bec02cae9b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae8a7a9-6902-424e-8e66-b107fccb361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import plot_ci, plot_ci_plus_heatmap\n",
    "from llamawrapper import load_tokenizer_only\n",
    "from eval_core import (\n",
    "    get_latents_from_hf_model, \n",
    "    get_latents_from_bnb_model, \n",
    "    get_latents_from_gptq_model, \n",
    "    get_latents_from_awq_model, \n",
    "    get_latents_from_rtn_model, \n",
    "    get_latents_from_mpq_model,\n",
    "    get_latents_from_mpqr_model,\n",
    ")\n",
    "# fix random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb93987",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "input_lang = 'en'\n",
    "target_lang = 'fr'\n",
    "model_size = '7b'\n",
    "custom_model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "single_token_only = False\n",
    "multi_token_only = False\n",
    "out_dir = './visuals'\n",
    "quant_type = \"mpq_rand\" # Can be among [\"rtn,\", \"hf\", \"gptq\", \"awq\", \"twq\", \"mpq\", \"mpqr\", \"twq_rand\", \"mpq_rand\", \"mpqr_rand\"]\n",
    "precision = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95475c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "if single_token_only and multi_token_only:\n",
    "    raise ValueError('single_token_only and multi_token_only cannot be True at the same time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1a3be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"./data/langs/\"\n",
    "df_en_fr = pd.read_csv(f'{prefix}{input_lang}/clean.csv').reindex()\n",
    "df_en_de = pd.read_csv(f'{prefix}{target_lang}/clean.csv').reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f081bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer_only(custom_model)\n",
    "# model = llama.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5686a8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for idx, word in enumerate(df_en_de['word_translation']):\n",
    "    if word in tokenizer.get_vocab() or '▁'+word in tokenizer.get_vocab():\n",
    "        count += 1\n",
    "        if multi_token_only:\n",
    "            df_en_de.drop(idx, inplace=True)\n",
    "    elif single_token_only:\n",
    "        df_en_de.drop(idx, inplace=True)\n",
    "\n",
    "print(f'for {target_lang} {count} of {len(df_en_de)} are single tokens')\n",
    "\n",
    "if input_lang == target_lang:\n",
    "    df_en_de_fr = df_en_de.copy()\n",
    "    df_en_de_fr.rename(columns={'word_original': 'en', \n",
    "                                f'word_translation': target_lang if target_lang != 'en' else 'en_tgt'}, \n",
    "                                inplace=True)\n",
    "else:\n",
    "    df_en_de_fr = df_en_de.merge(df_en_fr, on=['word_original'], suffixes=(f'_{target_lang}', f'_{input_lang}'))\n",
    "    df_en_de_fr.rename(columns={'word_original': 'en', \n",
    "                                f'word_translation_{target_lang}': target_lang if target_lang != 'en' else 'en_tgt', \n",
    "                                f'word_translation_{input_lang}': input_lang if input_lang != 'en' else 'en_in'}, \n",
    "                                inplace=True)\n",
    "# delete all rows where en is contained in de or fr\n",
    "if target_lang != 'en':\n",
    "    for i, row in df_en_de_fr.iterrows():\n",
    "        if row['en'].lower() in row[target_lang].lower():\n",
    "            df_en_de_fr.drop(i, inplace=True)\n",
    "\n",
    "print(f'final length of df_en_de_fr: {len(df_en_de_fr)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c0a52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_prefixes(token_str: str):\n",
    "    n = len(token_str)\n",
    "    tokens = [token_str[:i] for i in range(1, n+1)]\n",
    "    return tokens \n",
    "\n",
    "def add_spaces(tokens):\n",
    "    return ['▁' + t for t in tokens] + tokens\n",
    "\n",
    "def capitalizations(tokens):\n",
    "    return list(set(tokens))\n",
    "\n",
    "def unicode_prefix_tokid(zh_char = \"云\", tokenizer=tokenizer):\n",
    "    start = zh_char.encode().__str__()[2:-1].split('\\\\x')[1]\n",
    "    unicode_format = '<0x%s>'\n",
    "    start_key = unicode_format%start.upper()\n",
    "    if start_key in tokenizer.get_vocab():\n",
    "        return tokenizer.get_vocab()[start_key]\n",
    "    return None\n",
    "\n",
    "def process_tokens(token_str: str, tokenizer, lang):\n",
    "    with_prefixes = token_prefixes(token_str)\n",
    "    with_spaces = add_spaces(with_prefixes)\n",
    "    with_capitalizations = capitalizations(with_spaces)\n",
    "    final_tokens = []\n",
    "    for tok in with_capitalizations:\n",
    "        if tok in tokenizer.get_vocab():\n",
    "            final_tokens.append(tokenizer.get_vocab()[tok])\n",
    "    if lang in ['zh', 'ru']:\n",
    "        tokid = unicode_prefix_tokid(token_str, tokenizer)\n",
    "        if tokid is not None:\n",
    "            final_tokens.append(tokid)\n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa7bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id2voc = {id:voc for voc, id in tokenizer.get_vocab().items()}\n",
    "def get_tokens(token_ids, id2voc=id2voc):\n",
    "    return [id2voc[tokid] for tokid in token_ids]\n",
    "\n",
    "def compute_entropy(probas):\n",
    "    return (-probas*torch.log2(probas)).sum(dim=-1)\n",
    "\n",
    "lang2name = {'fr': 'Français', 'de': 'Deutsch', 'ru': 'Русский', 'en': 'English', 'zh': '中文'}\n",
    "def sample(df, ind, k=5, tokenizer=tokenizer, lang1='fr', lang2='de', lang_latent='en'):\n",
    "    df = df.reset_index(drop=True)\n",
    "    temp = df[df.index!=ind]\n",
    "    sample = pd.concat([temp.sample(k-1), df[df.index==ind]], axis=0)\n",
    "    prompt = \"\"\n",
    "    for idx, (df_idx, row) in enumerate(sample.iterrows()):\n",
    "        if idx < k-1:\n",
    "            prompt += f'{lang2name[lang1]}: \"{row[lang1]}\" - {lang2name[lang2]}: \"{row[lang2]}\"\\n'\n",
    "        else:\n",
    "            prompt += f'{lang2name[lang1]}: \"{row[lang1]}\" - {lang2name[lang2]}: \"'\n",
    "            in_token_str = row[lang1]\n",
    "            out_token_str = row[lang2]\n",
    "            out_token_id = process_tokens(out_token_str, tokenizer, lang2)\n",
    "            latent_token_str = row[lang_latent]\n",
    "            latent_token_id = process_tokens(latent_token_str, tokenizer, 'en')\n",
    "            intersection = set(out_token_id).intersection(set(latent_token_id))\n",
    "            if len(out_token_id) == 0 or len(latent_token_id) == 0:\n",
    "                yield None\n",
    "            if lang2 != 'en' and len(intersection) > 0:\n",
    "                yield None\n",
    "            yield {'prompt': prompt, \n",
    "                'out_token_id': out_token_id, \n",
    "                'out_token_str': out_token_str,\n",
    "                'latent_token_id': latent_token_id, \n",
    "                'latent_token_str': latent_token_str, \n",
    "                'in_token_str': in_token_str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500641f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for ind in tqdm(range(len(df_en_de_fr))):\n",
    "    d = next(sample(df_en_de_fr, ind, lang1=input_lang, lang2=target_lang))\n",
    "    if d is None:\n",
    "        continue\n",
    "    dataset.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5753d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if quant_type in [\"mpq_rand\", \"mpqr_rand\", \"twq_rand\"]:\n",
    "    path = \"../neurons/random.neuron.pth\"\n",
    "    target_neurons = torch.load(path)\n",
    "    quant_type = quant_type[:-5]\n",
    "    model_label = f\"{quant_type}_{precision}bit_random\"\n",
    "elif quant_type in [\"mpq\", \"mpqr\", \"twq\"]:\n",
    "    path = \"../neurons/combined.neuron.pth\"\n",
    "    target_neurons = torch.load(path)\n",
    "    model_label = f\"{quant_type}_{precision}bit_combined\"\n",
    "else:\n",
    "    target_neurons = None\n",
    "    model_label = f\"{quant_type}_{precision}bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d786ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset)\n",
    "os.makedirs(f'{os.path.join(out_dir, model_label)}/translation', exist_ok=True)\n",
    "if single_token_only:\n",
    "    df.to_csv(f'{os.path.join(out_dir, model_label)}/translation/{model_size}_{input_lang}_{target_lang}_dataset_single_token.csv', index=False)\n",
    "elif multi_token_only:\n",
    "    df.to_csv(f'{os.path.join(out_dir, model_label)}/translation/{model_size}_{input_lang}_{target_lang}_dataset_multi_token.csv', index=False)\n",
    "else:\n",
    "    df.to_csv(f'{os.path.join(out_dir, model_label)}/translation/{model_size}_{input_lang}_{target_lang}_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d4edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "match quant_type:\n",
    "    case \"hf\": latent_getter = get_latents_from_hf_model\n",
    "    case \"bnb\": latent_getter = get_latents_from_bnb_model\n",
    "    case \"gptq\": latent_getter = get_latents_from_gptq_model \n",
    "    case \"awq\": latent_getter = get_latents_from_awq_model if precision == 4 else get_latents_from_hf_model\n",
    "    case \"rtn\": latent_getter = get_latents_from_rtn_model\n",
    "    case \"twq\": latent_getter = get_latents_from_hf_model\n",
    "    case \"mpq\": latent_getter = get_latents_from_mpq_model\n",
    "    case \"mpqr\": latent_getter = get_latents_from_mpqr_model\n",
    "    case _: raise ValueError(\"Not yet implemented!\")\n",
    "        \n",
    "latent_token_prob, out_token_prob, entropy, energy, latents = latent_getter(\n",
    "    dataset, custom_model, precision=precision, targets=target_neurons)\n",
    "latent_token_probs = latent_token_prob.cpu()\n",
    "out_token_probs = out_token_prob.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ef0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "size2tik = {'7b': 5, '13b': 5, '70b': 10, \"tiny\": 5}\n",
    "fig, ax, ax2 = plot_ci_plus_heatmap(latent_token_probs, entropy, 'en', color='tab:orange', tik_step=size2tik[model_size], do_colorbar=True, #, do_colorbar=(model_size=='70b'),\n",
    "nums=[.99, 0.18, 0.025, 0.6])\n",
    "if target_lang != 'en':\n",
    "    plot_ci(ax2, out_token_probs, target_lang, color='tab:blue', do_lines=False)\n",
    "ax2.set_xlabel('layer')\n",
    "ax2.set_ylabel('probability')\n",
    "if model_size == '7b' or model_size == 'tiny':\n",
    "    ax2.set_xlim(0, out_token_probs.shape[1]+1)\n",
    "else:\n",
    "    ax2.set_xlim(0, round(out_token_probs.shape[1]/10)*10+1)\n",
    "ax2.set_ylim(0, 1)\n",
    "# make xticks start from 1\n",
    "# put legend on the top left\n",
    "ax2.legend(loc='upper left')\n",
    "os.makedirs(f'{os.path.join(out_dir, model_label)}/translation', exist_ok=True)\n",
    "if single_token_only:\n",
    "    plt.savefig(f'{os.path.join(out_dir, model_label)}/translation/{model_size}_{input_lang}_{target_lang}_probas_ent_single_token.pdf', dpi=300, bbox_inches='tight')\n",
    "elif multi_token_only:\n",
    "    plt.savefig(f'{os.path.join(out_dir, model_label)}/translation/{model_size}_{input_lang}_{target_lang}_probas_ent_multi_token.pdf', dpi=300, bbox_inches='tight')\n",
    "else:\n",
    "    plt.savefig(f'{os.path.join(out_dir, model_label)}/translation/{model_size}_{input_lang}_{target_lang}_probas_ent.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c50e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "size2tik = {'7b': 5, '13b': 5, '70b': 10, \"tiny\": 5}\n",
    "\n",
    "fig, ax2 = plt.subplots(figsize=(5,3))\n",
    "plot_ci(ax2, energy.cpu(), 'energy', color='tab:green', do_lines=True, tik_step=size2tik[model_size])\n",
    "ax2.set_xlabel('layer')\n",
    "ax2.set_ylabel('energy')\n",
    "if model_size == '7b' or model_size == 'tiny':\n",
    "    ax2.set_xlim(0, out_token_probs.shape[1]+1)\n",
    "else:\n",
    "    ax2.set_xlim(0, round(out_token_probs.shape[1]/10)*10+1)\n",
    "os.makedirs(f'{os.path.join(out_dir, model_label)}/translation', exist_ok=True)\n",
    "if single_token_only:\n",
    "    plt.savefig(f'{os.path.join(out_dir, model_label)}/translation/{model_size}_{input_lang}_{target_lang}_probas_ent_single_token.pdf', dpi=300, bbox_inches='tight')\n",
    "elif multi_token_only:\n",
    "    plt.savefig(f'{os.path.join(out_dir, model_label)}/translation/{model_size}_{input_lang}_{target_lang}_probas_ent_multi_token.pdf', dpi=300, bbox_inches='tight')\n",
    "else:\n",
    "    plt.savefig(f'{os.path.join(out_dir, model_label)}/translation/{model_size}_{input_lang}_{target_lang}_energy.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6938bbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"_rtn\" if quant_type == \"rtn\" else \"\"\n",
    "torch.save(latent_token_probs, f'{os.path.join(out_dir, model_label)}/translation/{model_label}_{input_lang}_{target_lang}_latent_probs.pt')\n",
    "torch.save(out_token_probs, f'{os.path.join(out_dir, model_label)}/translation/{model_label}_{input_lang}_{target_lang}_out_probs.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
