{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e266eca-b017-461f-9be4-bec02cae9b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae8a7a9-6902-424e-8e66-b107fccb361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from scipy.stats import bootstrap\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import plot_ci, plot_ci_plus_heatmap\n",
    "from llamawrapper import load_tokenizer_only\n",
    "from eval_core import (\n",
    "    get_latents_from_bnb_model, \n",
    "    get_latents_from_gptq_model, \n",
    "    get_latents_from_awq_model, \n",
    "    get_latents_from_rtn_model, \n",
    "    get_latents_from_hf_model,\n",
    "    get_latents_from_mpq_model,\n",
    "    get_latents_from_mpqr_model,\n",
    ")\n",
    "\n",
    "# fix random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb93987",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "target_lang = 'fr'\n",
    "model_size = '7b'\n",
    "custom_model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "single_token_only = False\n",
    "multi_token_only = False\n",
    "out_dir = './visuals'\n",
    "quant_type = \"mpq_rand\" # Can be among [\"rtn,\", \"hf\", \"gptq\", \"awq\", \"twq\", \"mpq\", \"mpqr\", \"twq_rand\", \"mpq_rand\", \"mpqr_rand\"]\n",
    "precision = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1a3be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"./data/langs/\"\n",
    "df_en_de = pd.read_csv(f'{prefix}{target_lang}/clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f081bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer_only(custom_model)\n",
    "# model = llama.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c0a52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_prefixes(token_str: str):\n",
    "    n = len(token_str)\n",
    "    tokens = [token_str[:i] for i in range(1, n+1)]\n",
    "    return tokens \n",
    "\n",
    "def add_spaces(tokens):\n",
    "    return ['▁' + t for t in tokens] + tokens\n",
    "\n",
    "def capitalizations(tokens):\n",
    "    return list(set(tokens))\n",
    "\n",
    "def unicode_prefix_tokid(zh_char = \"云\", tokenizer=tokenizer):\n",
    "    start = zh_char.encode().__str__()[2:-1].split('\\\\x')[1]\n",
    "    unicode_format = '<0x%s>'\n",
    "    start_key = unicode_format%start.upper()\n",
    "    if start_key in tokenizer.get_vocab():\n",
    "        return tokenizer.get_vocab()[start_key]\n",
    "    return None\n",
    "\n",
    "def process_tokens(token_str: str, tokenizer, lang):\n",
    "    with_prefixes = token_prefixes(token_str)\n",
    "    with_spaces = add_spaces(with_prefixes)\n",
    "    with_capitalizations = capitalizations(with_spaces)\n",
    "    final_tokens = []\n",
    "    for tok in with_capitalizations:\n",
    "        if tok in tokenizer.get_vocab():\n",
    "            final_tokens.append(tokenizer.get_vocab()[tok])\n",
    "    if lang in ['zh', 'ru']:\n",
    "        tokid = unicode_prefix_tokid(token_str, tokenizer)\n",
    "        if tokid is not None:\n",
    "            final_tokens.append(tokid)\n",
    "    return final_tokens\n",
    "\n",
    "id2voc = {id:voc for voc, id in tokenizer.get_vocab().items()}\n",
    "def get_tokens(token_ids, id2voc=id2voc):\n",
    "    return [id2voc[tokid] for tokid in token_ids]\n",
    "\n",
    "def compute_entropy(probas):\n",
    "    return (-probas*torch.log2(probas)).sum(dim=-1)\n",
    "\n",
    "lang2name = {'fr': 'Français', 'de': 'Deutsch', 'ru': 'Русский', 'en': 'English', 'zh': '中文'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeca7bf",
   "metadata": {},
   "source": [
    "# Gap texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d53abf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"blank_prompt_translation_masked\"\n",
    "dataset_gap = []\n",
    "n_skip = 2\n",
    "\n",
    "for idx, (idx_df, row) in tqdm(enumerate(df_en_de.iterrows())):\n",
    "    prompt_template = f''\n",
    "    indices = set(list(range(len(df_en_de)))) - set([idx])\n",
    "    idx_examples = np.random.choice(list(indices), n_skip, replace=False)\n",
    "    prompt_template += f'{df_en_de[key][idx_examples[0]]}\\n'\n",
    "    prompt_template += f'{df_en_de[key][idx_examples[1]]}\\n' \n",
    "\n",
    "    # get tok sets and kick out if intersection\n",
    "    out_token_str = row['word_translation']\n",
    "    latent_token_str = row['word_original']\n",
    "    out_token_id = process_tokens(out_token_str, tokenizer, target_lang)\n",
    "    latent_token_id = process_tokens(latent_token_str, tokenizer, 'en')\n",
    "    intersection = set(out_token_id).intersection(set(latent_token_id))\n",
    "    if len(out_token_id) == 0 or len(latent_token_id) == 0:\n",
    "        continue\n",
    "    if target_lang != 'en' and len(intersection) > 0:\n",
    "        continue \n",
    "    if target_lang == 'zh':\n",
    "        prompt = row[key].split(\"：\")[0]+\": \\\"\"\n",
    "    else: \n",
    "        prompt = row[key].split(\":\")[0]+\": \\\"\"\n",
    "    dataset_gap.append({\n",
    "        'prompt': prompt_template + prompt,\n",
    "        'out_token_id': out_token_id,\n",
    "        'out_token_str': out_token_str,\n",
    "        'latent_token_id': latent_token_id,\n",
    "        'latent_token_str': latent_token_str,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f787125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8c342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gap = pd.DataFrame(dataset_gap)\n",
    "print(df_gap['prompt'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a0060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if quant_type in [\"mpq_rand\", \"mpqr_rand\", \"twq_rand\"]:\n",
    "    path = \"../neurons/random.neuron.pth\"\n",
    "    target_neurons = torch.load(path)\n",
    "    quant_type = quant_type[:-5]\n",
    "    model_label = f\"{quant_type}_{precision}bit_random\"\n",
    "elif quant_type in [\"mpq\", \"mpqr\", \"twq\"]:\n",
    "    path = \"../neurons/combined.neuron.pth\"\n",
    "    target_neurons = torch.load(path)\n",
    "    model_label = f\"{quant_type}_{precision}bit_combined\"\n",
    "else:\n",
    "    target_neurons = None\n",
    "    model_label = f\"{quant_type}_{precision}bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd671dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{os.path.join(out_dir, model_label)}/cloze', exist_ok=True)\n",
    "df_gap.to_csv(f'{os.path.join(out_dir, model_label)}/cloze/{target_lang}_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc2e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "match quant_type:\n",
    "    case \"hf\": latent_getter = get_latents_from_hf_model\n",
    "    case \"bnb\": latent_getter = get_latents_from_bnb_model\n",
    "    case \"gptq\": latent_getter = get_latents_from_gptq_model \n",
    "    case \"awq\": latent_getter = get_latents_from_awq_model if precision == 4 else get_latents_from_hf_model\n",
    "    case \"rtn\": latent_getter = get_latents_from_rtn_model\n",
    "    case \"twq\": latent_getter = get_latents_from_hf_model\n",
    "    case \"mpq\": latent_getter = get_latents_from_mpq_model\n",
    "    case \"mpqr\": latent_getter = get_latents_from_mpqr_model\n",
    "    case _: raise ValueError(\"Not yet implemented!\")\n",
    "        \n",
    "latent_token_prob, out_token_prob, entropy, energy, latents = latent_getter(dataset_gap, custom_model, precision=precision, targets=target_neurons)\n",
    "latent_token_probs = latent_token_prob.cpu()\n",
    "out_token_probs = out_token_prob.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6266c8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "size2tik = {'tiny': 5, '7b': 5, '13b': 5, '70b': 10}\n",
    "\n",
    "fig, ax, ax2 = plot_ci_plus_heatmap(latent_token_probs, entropy, 'en', color='tab:orange', tik_step=size2tik[model_size], do_colorbar=True,\n",
    "nums=[.99, 0.18, 0.025, 0.6])\n",
    "if target_lang != 'en':\n",
    "    plot_ci(ax2, out_token_probs, target_lang, color='tab:blue', do_lines=False)\n",
    "ax2.set_xlabel('layer')\n",
    "ax2.set_ylabel('probability')\n",
    "if model_size == '7b':\n",
    "    ax2.set_xlim(0, out_token_probs.shape[1]+1)\n",
    "else:\n",
    "    ax2.set_xlim(0, round(out_token_probs.shape[1]/10)*10+1)\n",
    "ax2.set_ylim(0, 1)\n",
    "# put legend on the top left\n",
    "ax2.legend(loc='upper left')\n",
    "os.makedirs(f'{os.path.join(out_dir, model_label)}/cloze', exist_ok=True)\n",
    "plt.savefig(f'{os.path.join(out_dir, model_label)}/cloze/{model_size}_{target_lang}_probas_ent.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33806161",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax2 = plt.subplots(figsize=(5,3))\n",
    "plot_ci(ax2, energy, 'energy', color='tab:green', do_lines=True, tik_step=size2tik[model_size])\n",
    "ax2.set_xlabel('layer')\n",
    "ax2.set_ylabel('energy')\n",
    "if model_size == '7b':\n",
    "    ax2.set_xlim(0, out_token_probs.shape[1]+1)\n",
    "else:\n",
    "    ax2.set_xlim(0, round(out_token_probs.shape[1]/10)*10+1)\n",
    "os.makedirs(f'{os.path.join(out_dir, model_label)}/cloze', exist_ok=True)\n",
    "plt.savefig(f'{os.path.join(out_dir, model_label)}/cloze/{model_size}_{target_lang}_energy.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfb206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(latent_token_probs, f'{os.path.join(out_dir, model_label)}/cloze/{model_label}_{target_lang}_latent_probs.pt')\n",
    "torch.save(out_token_probs, f'{os.path.join(out_dir, model_label)}/cloze/{model_label}_{target_lang}_out_probs.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
